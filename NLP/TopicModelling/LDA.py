import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

data = [' Most shark attacks occur about 10 feet from the beach since that is where the people are',
        'the efficiency with which he paired the socks in the drawer was quite admirable',
        'carol drank the blood as if she were a vampire',
        'giving directions that the mountains are to the west only works when you can see them',
        'the sign said there was road work ahead so he decided to speed up',
        'the gruff old man sat in the back of the bait shop grumbling to himself as he scooped out a handful of worms']

countvec = CountVectorizer(stop_words='english')
countvec_fit = countvec.fit_transform(data)

lda = LatentDirichletAllocation(n_components=2, random_state=42)
lda_fit = lda.fit_transform(countvec_fit)

for idx, topic in enumerate(lda.components_):
    print(f"Topic {idx}:")
    print([countvec.get_feature_names_out()[i] for i in topic.argsort()[-5:]])


# How does LDA work?
# LDA works by assuming that each document is a mixture of topics, and each topic is a distribution over words.
# It infers the topics from the documents and assigns probabilities to each word in the vocabulary for each topic.
# The model uses a generative process to create documents, where each document is generated by first selecting a topic distribution and then selecting words based on that distribution.
# LDA is particularly useful for discovering hidden thematic structure in large collections of documents.
# LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.
# It is commonly used for topic modeling in text data.


# Topic is latent within the data.
# We must discover the topics from the data.
# A document can contain words from multiple topics.
# LDA assumes that documents are mixtures of topics, where each topic is a distribution over words.
# It infers the topics from the documents and assigns probabilities to each word in the vocabulary for each topic.
# LDA is particularly useful for discovering hidden thematic structure in large collections of documents.

# LDA works an iterative process:
# 1. Initialize the model with a specified number of topics. K
# 2. Randomly assign each word in the documents to one of the K topics.
# 3. For each word in each document, update the topic assignment based on the current topic distributions and word distributions.
# 4. Repeat step 3 until convergence, where the topic assignments stabilize.

# LDA checks how frequently words appear in documents and how often they appear together.
# It uses this information to group words into topics based on their co-occurrence patterns.
# LDA is particularly useful for discovering hidden thematic structure in large collections of documents.
# LDA is a powerful technique for topic modeling, allowing us to uncover the underlying themes in a collection of documents.
# LDA is particularly useful for discovering hidden thematic structure in large collections of documents.

# LDA corrects the assignments of words to topics based on the co-occurrence patterns of words in the documents.
# LDA is particularly useful for discovering hidden thematic structure in large collections of documents.





